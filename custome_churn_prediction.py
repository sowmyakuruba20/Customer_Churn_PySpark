# -*- coding: utf-8 -*-
"""The Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bQblqQWBUtTOEqOUuZ0Qz4D6ybJad5H1

# **Welcome To the Notebook**

### **Task 1 - Loading our data**

Installing the pyspark using pip
"""

!pip install pyspark

"""Importing Modules"""

# importing spark session
from pyspark.sql import SparkSession

# data visualization modules
import matplotlib.pyplot as plt
import plotly.express as px

# pandas module
import pandas as pd

# pyspark SQL functions
from pyspark.sql.functions import col, when, count, udf

# pyspark data preprocessing modules
from pyspark.ml.feature import Imputer, StringIndexer, VectorAssembler, StandardScaler

# pyspark data modeling and model evaluation modules
from pyspark.ml.classification import DecisionTreeClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator

"""Building our Spark Session"""

spark = SparkSession.builder.appName("Customer_Churn_Prediction").getOrCreate()
spark

"""Loading our data"""

data = spark.read.format("csv").option("inferSchema", "true").option("header", "true").load("/content/dataset.csv")
data.show(5)

"""Print the data schema to check out the data types"""

data.printSchema()

"""Get the data dimension"""

data.count()

data.columns
len(data.columns)

"""### **Task 2 - Exploratory Data Analysis**
- Distribution Analysis
- Correlation Analysis
- Univariate Analysis
- Finding Missing values

Let's define some lists to store different column names with different data types.
"""

data.dtypes

#lets consider all string as categorial column
categorical_columns = [name for name,typ in data.dtypes if typ in ["string"]]


#lets consider the int and double as numerical columns
numerical_columns = [name for name,typ in data.dtypes if typ in ["int","double"]]

data.select(categorical_columns).show(5)

data.select(numerical_columns).show(5)

"""Let's get all the numerical features and store them into a pandas dataframe."""

df = data.select(numerical_columns).toPandas()
df.head()

"""Let's create histograms to analyse the distribution of our numerical columns."""

fig = plt.figure(figsize=(15,10))
ax = fig.gca()
df.hist(ax=ax, bins=20)
plt.show()

df.tenure.describe()

"""There is an outlier in the tenure column if you compare the mean and max values

Let's generate the correlation matrix
"""

df.corr()

"""Let's check the unique value count per each categorical variables"""

cat_df = data.select(categorical_columns).toPandas()
cat_df.head()

cat_df.Contract.value_counts()

for column in categorical_columns:
  data.groupBy(column).count().show()

"""Let's find number of null values in all of our dataframe columns"""

data.select(count(when(col("Churn").isNull(),"Churn")).alias("Churn")).show()

for column in data.columns:
  data.select(count(when(col(column).isNull(),column)).alias(column)).show()

"""In the TotalCharges column there are 11 missing values

### **Task 3 - Data Preprocessing**
- Handling the missing values
- Removing the outliers

**Handling the missing values** <br>
Let's create a list of column names with missing values
"""

columns_with_missing_values = ["TotalCharges"]

"""Creating our Imputer"""

imputer = Imputer(inputCols=columns_with_missing_values, outputCols=columns_with_missing_values).setStrategy("mean")

"""Use Imputer to fill the missing values"""

imputer = imputer.fit(data)
data = imputer.transform(data)

"""Let's check the missing value counts again"""

data.select(count(when(col("TotalCharges").isNull(),"TotalCharges")).alias("TotalCharges")).show()

"""**Removing the outliers** <br>
Let's find the customer with the tenure higher than 100
"""

data.select("*").where(data.tenure > 100).show()

"""Lets remove the outlier"""

print("Before removing the outlier: ", data.count())
data = data.filter(data.tenure < 100)
print("After removing the outlier: ", data.count())

"""### **Task 4 - Feature Preparation**
- Numerical Features
    - Vector Assembling
    - Numerical Scaling
- Categorical Features
    - String Indexing
    - Vector Assembling

- Combining the numerical and categorical feature vectors




**Feature Preparation - Numerical Features** <br>

`Vector Assembling --> Standard Scaling` <br>

**Vector Assembling** <br>
To apply our machine learning model we need to combine all of our numerical and categorical features into vectors. For now let's create a feature vector for our numerical columns.

"""

numerical_verctor_assembler = VectorAssembler(inputCols=numerical_columns, outputCol="numerical_features_vector")
data = numerical_verctor_assembler.transform(data)
data.show(5)

"""**Numerical Scaling** <br>
Let's standardize all of our numerical features.
"""

scaler = StandardScaler(inputCol="numerical_features_vector", outputCol="numerical_features_scaled", withStd= True, withMean= True)
data = scaler.fit(data).transform(data)
data.show(5)

"""**Feature Preperation - Categorical Features** <br>

`String Indexing --> Vector Assembling` <br>

**String Indexing** <br>
We need to convert all the string columns to numeric columns.
"""

categorical_columns_indexed = [name + "_Indexed" for name in categorical_columns]
categorical_columns_indexed

indexer = StringIndexer(inputCols=categorical_columns, outputCols=categorical_columns_indexed)
data = indexer.fit(data).transform(data)
data.show(5)

categorical_columns_indexed.remove("Churn_Indexed")

categorical_columns_indexed.remove("customerID_Indexed")

"""Let's combine all of our categorifal features in to one feature vector."""

categorical_vector_assembler = VectorAssembler(inputCols=categorical_columns_indexed, outputCol="categorical_features_vector")
data = categorical_vector_assembler.transform(data)
data.show(5)

final_vector_assembler = VectorAssembler(inputCols=["categorical_features_vector", "numerical_features_scaled"], outputCol="final_features_vector")
data = final_vector_assembler.transform(data)
data.show(5)

data.select(["final_features_vector","Churn_Indexed"]).show()

"""Now let's combine categorical and numerical feature vectors.

### **Task 5 - Model Training**
- Train and Test data splitting
- Creating our model
- Training our model
- Make initial predictions using our model

In this task, we are going to start training our model
"""

train, test = data.randomSplit([0.7,0.3], seed = 100)

train.count(), test.count()

"""Now let's create and train our desicion tree"""

train.show()

dt = DecisionTreeClassifier(featuresCol = 'final_features_vector', labelCol = 'Churn_Indexed', maxDepth = 6)
dtModel = dt.fit(train)

"""Let's make predictions on our test data"""

predictions_test = dtModel.transform(test)
predictions_test.select(["Churn", "prediction"]).show()

"""### **Task 6 - Model Evaluation**
- Calculating area under the ROC curve for the `test` set
- Calculating area under the ROC curve for the `training` set
- Hyper parameter tuning
"""

evaluator = BinaryClassificationEvaluator(labelCol="Churn_Indexed")
auc_test = evaluator.evaluate(predictions_test, {evaluator.metricName: "areaUnderROC"})
auc_test

"""Let's get the AUC for our `training` set"""

predictions_train = dtModel.transform(train)
auc_train = evaluator.evaluate(predictions_train, {evaluator.metricName: "areaUnderROC"})
auc_train

"""**Hyper parameter tuning**

Let's find the best `maxDepth` parameter for our DT model.
"""

def evaluate_dt(mode_params):
      test_accuracies = []
      train_accuracies = []

      for maxD in mode_params:
        # train the model based on the maxD
        decision_tree = DecisionTreeClassifier(featuresCol = 'final_features_vector', labelCol = 'Churn_Indexed', maxDepth = maxD)
        dtModel = decision_tree.fit(train)

        # calculating test error
        predictions_test = dtModel.transform(test)
        evaluator = BinaryClassificationEvaluator(labelCol="Churn_Indexed")
        auc_test = evaluator.evaluate(predictions_test, {evaluator.metricName: "areaUnderROC"})
        # recording the accuracy
        test_accuracies.append(auc_test)

        # calculating training error
        predictions_training = dtModel.transform(train)
        evaluator = BinaryClassificationEvaluator(labelCol="Churn_Indexed")
        auc_training = evaluator.evaluate(predictions_training, {evaluator.metricName: "areaUnderROC"})
        train_accuracies.append(auc_training)

      return(test_accuracies, train_accuracies)

"""Let's define `params` list to evaluate our model iteratively with differe maxDepth parameter.  """

maxDepths = [2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20]

test_acc, train_acc = evaluate_dt(maxDepths)
print(test_acc)
print(train_acc)

"""Let's visualize our results"""

df = pd.DataFrame({"maxDepth": maxDepths, "test_acc": test_acc, "train_acc": train_acc})
df

px.line(df,x = "maxDepth", y = ["train_acc", "test_acc"])

"""1. The best maxDepth for our prediction
model is 6.
2. The model is overfitted for maxDepth = 20
3. The model is underfitter for maxDepth = 5

### **7 - Model Deployment**
- Giving Recommendations using our model

We were asked to recommend a solution to reduce the customer churn.

Let's look at the important feartures in the decision tree
"""

feature_importance = dtModel.featureImportances
feature_importance

scores = [score for i,score in enumerate(feature_importance)]
scores

feature_importance

df = pd.DataFrame(scores, columns = ["score"], index = categorical_columns_indexed + numerical_columns)
df

px.bar(df, y = "score")

"""Let's create a bar chart to visualize the customer churn per contract type"""

df = data.groupBy("Contract", "Churn_Indexed").count().toPandas()
px.bar(df, x = "Contract", y = "count", color = "Churn_Indexed")

"""The bar chart displays the number of churned customers based on their contract type. It is evident that customers with a "Month-to-month" contract have a higher churn rate compared to those with "One year" or "Two year" contracts. As a recommendation, the telecommunication company could consider offering incentives or discounts to encourage customers with month-to-month contracts to switch to longer-term contracts."""